\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{fancyvrb}
\usepackage{enumerate}
\usepackage{pgfplots}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}
\usepackage[format=hang,labelsep=quad]{caption}
\usepackage{subfig}
\usepackage{array}
\usepackage{multirow}

\renewcommand\thesubfigure{\roman{subfigure}}


\begin{document}
\noindent\large{Math 5365}\\
\large{Data Mining 1}\\
\large{Homework 15}\\
\large{Mary Barker}
\doublespace
\begin{enumerate}
\item 
 Compare the accuracies of bagging, boosting, and random forests on the wdbc 
 data set, using 70\% of the data as training data and 30\% as testing data.

 The respective accuracies and runtimes of the three methods are shown in the table below.

\begin{center}
\begin{tabular}{c|c|c}
Method & Accuracy & runtime \tabularnewline\hline
Bagging & 98.24561\% & 21.354 seconds\\
Forest & 99.4152\% & 0.268 seconds \\
Boosting & 99.4152\% & 21.478 seconds
\\ \hline
\end{tabular}
\end{center}

All three methods displayed excellent accuracy rates. However, the Forest model and the 
boosting method had the highest accuracy. In addition, the forest model performed better 
with runtime. Therefore the best method is the fastest and tied for most accurate in 
forest. The boosting method is the second fastest and also the most accurate. 
\end{enumerate}
\begin{Verbatim}[numbers=left]
#Data Mining hw 15

# Compare the accuracies of bagging, boosting, and random forests on the wdbc 
# data set, using 70% of the data as training data and 30% as testing data.
library(adabag)
library(randomForest)

source('~/Dropbox/Tarleton/data_mining/class_notes/extras.R')
source('~/Dropbox/Tarleton/data_mining/generic_functions/dataset_ops.R')
wdbc <- read.csv('~/Dropbox/Tarleton/data_mining/dfiles/wdbc.data', 
                  header=F, sep=',')
wdbc <- wdbc[,-1]
splitset <- splitdata(wdbc,0.7,F)
train <- splitset$train

bag1 <- proc.time()
baggingmodel <- bagging(V2~., wdbc[train,])
bagpred <- predict(baggingmodel,wdbc[-train,],type='class')
bagacc <- confmatrix(bagpred$class,wdbc$V2[-train])$accuracy
bag2 <- proc.time()

forest1 <- proc.time()
forestmodel <- randomForest(V2~.,wdbc[train,])
forestpred <- predict(forestmodel,wdbc[-train,])
forestacc <- confmatrix(forestpred,wdbc$V2[-train])$accuracy
forest2 <- proc.time()

boost1 <- proc.time()
boostingmodel <- boosting(V2~., wdbc[train,])
boostpred <- predict(boostingmodel,wdbc[-train,],type='class')
boostacc <- 1 - boostpred$error
boost2 <- proc.time()
\end{Verbatim}
\end{document}
