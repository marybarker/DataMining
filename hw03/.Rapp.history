# this program has a function that accepts a vector of arbitrary length and computes#
# the Gini impurity, entropy and the classification error measures. #
#
# checks that the set of values does add up to 1.#
# otherwise we're missing one or more sets.  #
check_add_to_1 = function(v1){#
  value = sum(v1)#
  v2 = v1#
  if(value < 1){#
  	v2 = rep(0, length(v1) + 1)#
    v2 = c(v1, 1 - value)#
    value = 0#
  }#
  if(value == 1){#
  	v2 = v1#
  }#
  return(v2)#
}#
#
gini_eval = function(distribution_v){#
  v = check_add_to_1(distribution_v)#
  gini = 1 - sum(v * v)#
  return(gini)#
}#
#
entropy_eval = function(distribution_v){#
  # create a temporary array that has same entries as#
  # distribution_v, except where distribution_v = 0. #
  # there tmp is 1 so I won't get nan computing #
  # log(tmp) (want the result to be 0 anyway)#
  v = check_add_to_1(distribution_v)#
  tmp = (v == 0) * 1#
  tmp = tmp + v#
#
  entropy = -1 * sum(tmp * (log(tmp) / log(2)))#
  return(entropy)#
}#
#
class_error_eval = function(distribution_v){#
  v = check_add_to_1(distribution_v)#
  class_error = 1 - max(v)#
  return(class_error)#
}#
#
x = seq(from=0,to=1,by=0.01)#
y1 <- rep(0, 101)#
y2 <- rep(0, 101)#
y3 <- rep(0, 101)#
for(i in 1:100){#
  y1[i] = gini_eval(x[i])#
}#
for(i in 1:100){#
  y2[i] = entropy_eval(x[i])#
}#
for(i in 1:100){#
  y3[i] = class_error_eval(x[i])#
}#
plot(x, y1, type='l',col='black', ylim=c(0,1))#
lines(x, y2, type='l',col='green')#
lines(x, y3, type='l',col='blue')#
#
####################################################
######   TEST FUNCTIONS WITH SAMPLE VECTOR   #######
####################################################
#
#class_distribution_vector = c(0.0, 0.3, 0.3, 0.4)#
#mygini =  gini_eval(class_distribution_vector)#
#print(' ')#
#print("============================")#
#print(paste("Gini measure: ",mygini))#
#print("============================")#
#print(' ')#
#
#myentropy =  entropy_eval(class_distribution_vector)#
#print(' ')#
#print("============================")#
#print(paste("entropy measure: ",myentropy))#
#print("============================")#
#print(' ')#
#
#myentropy1 =  entropy_eval1(class_distribution_vector)#
#print(' ')#
#print("============================")#
#print(paste("entropy1 measure: ",myentropy1))#
#print("============================")#
#print(' ')#
#
#myclass_error =  class_error_eval(class_distribution_vector)#
#print(' ')#
#print("============================")#
#print(paste("Gini measure: ",myclass_error))#
#print("============================")#
#print(' ')
p = kyphosis#
presentp = (p == "Present")*1#
presentp
##kyphosis {rpart}#
#it is a data frame with 81 rows, 4 columns representing data on children #
#who have had corrective spinal surgery.#
##
#columns are as follows:#
#+ Kyphosis#
#+ + + A factor with levels 'absent' 'present' indicating if a kyphosis was #
#+ + + present after the operation#
##
##
#+ Age#
#+ + + in months#
##
#+ Number #
#+ + + the number of vertebrae involved#
##
#+ Start#
#+ + + The number of the first (topmost) vertebra operated on.#
##
#examples:#
library(rpart)#
library(rpart.plot)#
library(party)#
library(treemap)#
#head(kyphosis)#
attach(kyphosis)#
#fit <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis)#
#fit2 <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis, parms = #
#              list(prior = c(0.65, 0.35), split = "information"))#
#fit3 <- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis,  #
#              control = rpart.control(cp = 0.05))#
#par(mfrow = c(1, 2), xpd = TRUE)#
#plot(fit, main="Kyphosis ~ Age + Number + Start")#
#text(fit, use.n = TRUE)#
#plot(fit2, main="Kyphosis ~ Age + Number + Start")#
#text(fit2, use.n = TRUE)#
#
#as.numeric(Kyphosis)#
#as.numeric(Start)#
#
ktree=rpart(Kyphosis~Age+Number+Start,data=kyphosis)#
#
plot(ktree,main="Kyphosis~Age+Number+Start")#
predkyphos = predict(ktree,newdata=kyphosis,type="class")#
confusionmatrix=table(Kyphosis,predkyphos)#
#
accuracy = sum(diag(confusionmatrix))/sum(confusionmatrix)#
#
#ktree2=ctree(Kyphosis~.,data=kyphosis)#
ktree2=ctree(Kyphosis~Age+Number+Start,data=kyphosis)#
plot(ktree2,type='simple')#
predkyphos2 = predict(ktree2,newdata=kyphosis)#
confusionmatrix=table(Kyphosis,predkyphos2)#
#
prp(ktree,type=4,extra=2)
p = kyphosis#
presentp = (p == "Present")*1#
presentp
p = kyphosis#
presentp = (p == "present")*1#
presentp
temp = presentp / 81
temp
p = ktree#
presentp = (p == "present")*1#
presentp#
#
temp = presentp / 81#
temp
p = kyphosis#
presentp = (p == "present")*1#
presentp#
#
temp = presentp / 81#
temp
n1 = c(1, 0)#
n2 = c(1, 0)#
n3 = c(0.14, 0.86)#
n4 = c(0.43, 0.57)#
n5 = c(0.58, 0.42)#
#
w_entropy = 0#
w_entropy = entropy_eval(n1) + entropy_eval(n2) + #
entropy_eval(n3) + entropy_eval(n4) + entropy_eval(n5)#
w_entropy
n1 = c(1)#
n2 = c(1)#
n3 = c(12/14)#
n4 = c(4/7)#
n5 = c(11/19)
w_entropy = 0#
w_entropy = entropy_eval(n1) + entropy_eval(n2) + entropy_eval(n3) + entropy_eval(n4) + entropy_eval(n5)#
w_entropy
w_entropy = n11 * entropy_eval(n1) + n22 * entropy_eval(n2) + n33 * entropy_eval(n3) + n44 * entropy_eval(n4) + n55 * entropy_eval(n5)#
w_entropy
n1 = c(1)#
n11 = 29/81#
n2 = c(1)#
n22 = 12/81#
n3 = c(12/14)#
n33 = 14/81#
n4 = c(4/7)#
n44 = 7 / 81#
n5 = c(11/19)#
n55 = 19/81#
w_entropy = 0#
w_entropy = n11 * entropy_eval(n1) + n22 * entropy_eval(n2) + n33 * entropy_eval(n3) + n44 * entropy_eval(n4) + n55 * entropy_eval(n5)#
w_entropy
n1 = c(0.421, 0.579)#
n11 = 19/81#
n2 = c(0.903, 0.097)#
n22 = 62/81#
w_entropy = entropy_eval(n1) + entropy_eval(n2)#
w_entropy
w_entropy = n11 * entropy_eval(n1) + n22 * entropy_eval(n2)#
w_entropy
prp(ktree,type=4,extra=102)
sum(presentp)
n1 = entropy_eval(c(2/5, 3/5))
n1
entropy_eval(c(0.7, 0.3))
entropy_eval(c(n1))
n1 = 64/81#
entropy_eval(c(n1))
ktree=rpart(Kyphosis~Number+Age+Start,data=kyphosis)
ktree2=ctree(Kyphosis~Number+Age+Start,data=kyphosis)
prp(ktree2,type=4,extra=102)
library(rpart)#
library(rpart.plot)#
library(party)#
library(treemap)
prp(ktree2,type=4,extra=102)
prp(ktree,type=4,extra=102)
